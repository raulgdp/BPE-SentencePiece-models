{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "toxic-traveler",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"\"\"\n",
    " De: monete_que_no_ve@lostresmonetes.net\n",
    " Enviado el: Jueves, 18 de noviembre de 2012 a las 13:22\n",
    " Para: torpedo@submarino.com\n",
    " Asunto: Re: Conquistar el mundo\n",
    " Hola.\n",
    " Aapfojewagf ajwa wjepofoisa jvgoisajigf jewapoijewagomsod moisjaoigjpoewijsn dsanigeaoi.\n",
    " Ajfpoijwafe sodvm osznfinewahaw eoansjgndsakjnglkjds.\n",
    " Alkjndszkng aigpiewannalkjndkjnlkjdznvns ln sa nfpoiewa npoinpewnpofn.\n",
    " N<sznvcknvknkxzvnoisajpoijewaoi jmsam lkvznapiunea engnal nfsl.\n",
    " De: torpedo@submarino.com\n",
    " Enviado el: Jueves, 18 de noviembre de 2012 a las 12:42\n",
    " Para: monete_que_no_ve@lostresmonetes.net\n",
    " Asunto: Re: Conquistar el mundo\n",
    " Hola.\n",
    " Aapfojewagf ajwa wjepofoisa jvgoisajigf jewapoijewagomsod moisjaoigjpoewijsn dsanigeaoi.\n",
    " Ajfpoijwafe sodvm osznfinewahaw eoansjgndsakjnglkjds.\n",
    " Alkjndszkng aigpiewannalkjndkjnlkjdznvns ln sa nfpoiewa npoinpewnpofn.\n",
    " N<sznvcknvknkxzvnoisajpoijewaoi jmsam lkvznapiunea engnal nfsl.\n",
    " De: monete_que_no_ve@lostresmonetes.net\n",
    " Enviado el: Jueves, 18 de noviembre de 2012 a las 11:57\n",
    " Para: torpedo@submarino.com\n",
    " Asunto: Re: Conquistar el mundo\n",
    " Hola.\n",
    " Aapfojewagf ajwa wjepofoisa jvgoisajigf jewapoijewagomsod moisjaoigjpoewijsn dsanigeaoi.\n",
    " Ajfpoijwafe sodvm osznfinewahaw eoansjgndsakjnglkjds.\n",
    " Alkjndszkng aigpiewannalkjndkjnlkjdznvns ln sa nfpoiewa npoinpewnpofn.\n",
    " N<sznvcknvknkxzvnoisajpoijewaoi jmsam lkvznapiunea engnal nfsl.\n",
    " De: torpedo@submarino.com\n",
    " Enviado el: Jueves, 18 de noviembre de 2012 a las 11:54\n",
    " Para: monete_que_no_ve@lostresmonetes.net\n",
    " Asunto: Re: Conquistar el mundo\n",
    " Hola.\n",
    " Aapfojewagf ajwa wjepofoisa jvgoisajigf jewapoijewagomsod moisjaoigjpoewijsn dsanigeaoi.\n",
    " Ajfpoijwafe sodvm osznfinewahaw eoansjgndsakjnglkjds.\n",
    " Alkjndszkng aigpiewannalkjndkjnlkjdznvns ln sa nfpoiewa npoinpewnpofn.\n",
    " N<sznvcknvknkxzvnoisajpoijewaoi jmsam lkvznapiunea engnal nfsl.\n",
    " De: monete_que_no_ve@lostresmonetes.net\n",
    " Enviado el: Jueves, 18 de noviembre de 2012 a las 09:15\n",
    " Para: torpedo@submarino.com\n",
    " Asunto: Conquistar el mundo\n",
    " Hola.\n",
    " Aapfojew mi primo el monete que no habla (Monete.que.no.habla@lostresmonetes.net) agf ajwa wjepofoisa jvgoisajigf.\n",
    " Para la ninia + w@pa del tuenti, hoygan (Monete_que_no_escucha@lostresmonetes.co.uk).\n",
    " Ajfpoijwafe sodvm osznfinewahaw eoansjgndsakjnglkjds.\n",
    " Alkjndszkng aigpiewannalkjndkjnlkjdznvns ln sa nfpoiewa npoinpewnpofn.\n",
    " Nsznvcknvknkxzvnoisajpoijewaoi jmsam lkvznapiunea engnal nfsl.\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "facial-better",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto1= \"\"\"La segunda ola de contagios en Chile, que comenzó en diciembre, \n",
    "se agravó en marzo tras las vacaciones de verano en esa región de Suramérica. Un poco más del 83 % de \n",
    "la población se encuentra en cuarentena total, incluida al capital,\n",
    "donde viven más de 7 millones de personas y todos los comercios no esenciales permanecen cerrados.\n",
    "\n",
    "Las nuevas medidas fueron adoptadas ante la realidad de las cifras: 23.677 muertos y más de un millón \n",
    "de infectados, de los cuales 42.794 se encuentran en etapa activa de la enfermedad y pueden contagiar. No obstante,\n",
    "Chile ha inoculado con una dosis a casi 7 millones de personas, lo que representa el 45 % de \n",
    "su población y lo sitúa como tercer país del mundo con más porcentaje de población vacunada.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "binding-sussex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2012 a las 13:22', '2012 a las 12:42', '2012 a las 11:57', '2012 a las 11:54', '2012 a las 09:15']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# 'w'=r'[a-zA-Z0-9_]'\n",
    "#r1 = re.findall(r'[\\w\\.-]+@[\\w\\.-]+',texto)\n",
    "#r2= re.findall(r'(^|[^a-zA-Z])([lL]a|[lL]as)[^a-zA-Z]', texto1)\n",
    "r3= re.findall(r'20[1-4][0-9]\\sa\\slas\\s[0-2][0-9]:[0-5][0-9]', texto)\n",
    "#a\\slas\\s[0-23]:[0-60]\n",
    "\n",
    "#print(r1)\n",
    "#print(len(r1))\n",
    "#print(r2)\n",
    "print(r3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-cornell",
   "metadata": {},
   "source": [
    "# Uso de finditer que permite iterar y da la posición del span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "noticed-antigua",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found <_sre.SRE_Match object; span=(55, 61), match='Jueves'>\n",
      "Found <_sre.SRE_Match object; span=(490, 496), match='Jueves'>\n",
      "Found <_sre.SRE_Match object; span=(953, 959), match='Jueves'>\n",
      "Found <_sre.SRE_Match object; span=(1388, 1394), match='Jueves'>\n",
      "Found <_sre.SRE_Match object; span=(1851, 1857), match='Jueves'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pattern = 'Jueves'\n",
    "\n",
    "for match in re.finditer(pattern, texto):\n",
    "    print('Found {!r}'.format(match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "written-sheffield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(31, 36), match='Chile'>\n",
      "31\n",
      "36\n",
      "(31, 36)\n"
     ]
    }
   ],
   "source": [
    "patron = \"Chile\"\n",
    "\n",
    "encontrado = re.search(patron,texto1)\n",
    "print(encontrado)\n",
    "print(encontrado.start())\n",
    "print(encontrado.end())\n",
    "print(encontrado.span())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-compilation",
   "metadata": {},
   "source": [
    "# Expresiones regulares para segmentar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "afraid-separate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['. U', '. N']\n"
     ]
    }
   ],
   "source": [
    "r3= re.findall(r'[.]+[\" \"][A-Z]', texto1)\n",
    "#a\\slas\\s[0-23]:[0-60]\n",
    "\n",
    "print(r3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-queensland",
   "metadata": {},
   "source": [
    "# Tokenizadores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "obvious-lover",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['No',\n",
       " 'dejemos',\n",
       " 'de',\n",
       " 'lado',\n",
       " 'la',\n",
       " 'educación',\n",
       " 'superior',\n",
       " ',',\n",
       " 'según',\n",
       " '@ASCUN1',\n",
       " ',',\n",
       " 'en',\n",
       " 'las',\n",
       " 'IES',\n",
       " 'privadas',\n",
       " 'la',\n",
       " 'disminución',\n",
       " 'de',\n",
       " 'matrícula',\n",
       " 'total',\n",
       " 'es',\n",
       " 'de',\n",
       " '10,1',\n",
       " '%',\n",
       " '(',\n",
       " '10.9',\n",
       " '%',\n",
       " 'en',\n",
       " 'pregrado',\n",
       " 'y',\n",
       " '5,6',\n",
       " '%',\n",
       " 'en',\n",
       " 'posgrado',\n",
       " ')',\n",
       " 'equivalente',\n",
       " 'a',\n",
       " '63.772',\n",
       " 'alumnos',\n",
       " 'en',\n",
       " '2',\n",
       " 'semestre',\n",
       " 'de',\n",
       " '2020',\n",
       " '.',\n",
       " 'Apoyo',\n",
       " 'a',\n",
       " 'las',\n",
       " 'IES',\n",
       " 'es',\n",
       " 'fundamental',\n",
       " '#LaEducacionPresencialEsVital']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "s0 = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "s1=  \"Comunicado de la Sociedad Colombiana de Pediatría. Toca volver al colegio. #LaEducacionPresencialEsVital @dario_maldonado @sandragarciajar @MSantamariaS @angelperezmar @patolinecamacho @julrubiano @AnzolaDeToro @villamizar @Brigittelgb\"\n",
    "s2=\"No dejemos de lado la educación superior, según @ASCUN1, en las IES privadas la disminución de matrícula total es de 10,1% (10.9% en pregrado y 5,6% en posgrado) equivalente a 63.772 alumnos en 2 semestre de 2020. Apoyo a las IES es fundamental #LaEducacionPresencialEsVital\"\n",
    "tknzr.tokenize(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "split-makeup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: twikenizer in /usr/local/lib/python3.6/dist-packages (1.0)\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (1.2.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Este', 'es', 'mi', '#hashtag']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip3 install twikenizer\n",
    "!pip3 install emoji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "driving-cholesterol",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'world', '😀', '#MondayMotivation']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import twikenizer as twk\n",
    "twk = twk.Twikenizer()\n",
    "tweet = 'Ciudadana, senadora de Colombia 🇨🇴 Partido Verde 🌻 Activista de las causas en que creo #ElCambioEsImparable! #MujeresBerracas💪#ColombiaSeRespeta!'\n",
    "t1=\"Hello world 😀 #MondayMotivation\"\n",
    "twk.tokenize(t1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-acrobat",
   "metadata": {},
   "source": [
    "# Tokenizador de Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "wound-manual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: spacy in /home/deep/.local/lib/python3.6/site-packages (3.0.5)\n",
      "Requirement already satisfied: jinja2 in /home/deep/.local/lib/python3.6/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/deep/.local/lib/python3.6/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/deep/.local/lib/python3.6/site-packages (from spacy) (0.7.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /home/deep/.local/lib/python3.6/site-packages (from spacy) (2.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /home/deep/.local/lib/python3.6/site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: setuptools in /home/deep/.local/lib/python3.6/site-packages (from spacy) (54.2.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /home/deep/.local/lib/python3.6/site-packages (from spacy) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /home/deep/.local/lib/python3.6/site-packages (from spacy) (3.7.4.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.19.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/deep/.local/lib/python3.6/site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/deep/.local/lib/python3.6/site-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/deep/.local/lib/python3.6/site-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/deep/.local/lib/python3.6/site-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /home/deep/.local/lib/python3.6/site-packages (from spacy) (2.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/deep/.local/lib/python3.6/site-packages (from spacy) (4.60.0)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /home/deep/.local/lib/python3.6/site-packages (from spacy) (1.7.3)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /home/deep/.local/lib/python3.6/site-packages (from spacy) (0.3.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/deep/.local/lib/python3.6/site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in /home/deep/.local/lib/python3.6/site-packages (from spacy) (8.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/deep/.local/lib/python3.6/site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/deep/.local/lib/python3.6/site-packages (from importlib-metadata>=0.20->spacy) (3.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/deep/.local/lib/python3.6/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /home/deep/.local/lib/python3.6/site-packages (from pathy>=0.3.5->spacy) (3.0.0)\n",
      "Requirement already satisfied: dataclasses<1.0,>=0.6 in /home/deep/.local/lib/python3.6/site-packages (from pathy>=0.3.5->spacy) (0.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/deep/.local/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/deep/.local/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/deep/.local/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/deep/.local/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: contextvars<3,>=2.4 in /home/deep/.local/lib/python3.6/site-packages (from thinc<8.1.0,>=8.0.2->spacy) (2.4)\n",
      "Requirement already satisfied: immutables>=0.9 in /home/deep/.local/lib/python3.6/site-packages (from contextvars<3,>=2.4->thinc<8.1.0,>=8.0.2->spacy) (0.14)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.6/dist-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/deep/.local/lib/python3.6/site-packages (from jinja2->spacy) (1.1.1)\n",
      "2021-04-14 21:26:08.600147: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "\n",
      "\u001b[38;5;1m✘ No compatible package found for 'en_dep_news_trf' (spaCy v3.0.5)\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip3 install spacy\n",
    "#!python3.6 -m spacy download en_core_news_sm\n",
    "#!python3.6 -m spacy download en_core_news_md\n",
    "#!python3.6 -m spacy download en_core_news_lg\n",
    "!python3.6 -m spacy download en_dep_news_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "automotive-collect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todos DET\n",
      "los DET\n",
      "programadores NOUN\n",
      "del ADP\n",
      "banco NOUN\n",
      "van VERB\n",
      "palante ADJ\n",
      "y CCONJ\n",
      "se PRON\n",
      "preparan VERB\n",
      "en ADP\n",
      "AT&T. PROPN\n",
      "Todos PROPN\n",
      "me PRON\n",
      "odian VERB\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "doc = nlp('Todos los programadores del banco van palante y se preparan en AT&T. Todos me odian.')\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-alfred",
   "metadata": {},
   "source": [
    "# Reconociendo números de teléfonos con spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "micro-seventh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Call', 'me', 'at', '(', '123', ')', '456', '789', 'or', '(', '123', ')', '456', '789', '!']\n",
      "(123) 456 789\n",
      "(123) 456 789\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"ORTH\": \"(\"}, {\"SHAPE\": \"ddd\"}, {\"ORTH\": \")\"}, {\"SHAPE\": \"ddd\"},\n",
    "           {\"ORTH\": \"-\", \"OP\": \"?\"}, {\"SHAPE\": \"ddd\"}]\n",
    "matcher.add(\"PHONE_NUMBER\", [pattern])\n",
    "\n",
    "doc = nlp(\"Call me at (123) 456 789 or (123) 456 789!\")\n",
    "print([t.text for t in doc])\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-latino",
   "metadata": {},
   "source": [
    "# Hashtags y emojis  con spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "thrown-lottery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HASHTAG #ElCambioEsImparable\n",
      "HASHTAG #MujeresBerracas\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = English()  # We only want the tokenizer, so no need to load a pipeline\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pos_emoji = [\"😀\", \"😃\", \"😂\", \"🤣\", \"😊\", \"😍\"]  # Positive emoji\n",
    "neg_emoji = [\"😞\", \"😠\", \"😩\", \"😢\", \"😭\", \"😒\"]  # Negative emoji\n",
    "\n",
    "# Add patterns to match one or more emoji tokens\n",
    "pos_patterns = [[{\"ORTH\": emoji}] for emoji in pos_emoji]\n",
    "neg_patterns = [[{\"ORTH\": emoji}] for emoji in neg_emoji]\n",
    "\n",
    "# Function to label the sentiment\n",
    "def label_sentiment(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    if doc.vocab.strings[match_id] == \"HAPPY\":  # Don't forget to get string!\n",
    "        doc.sentiment += 0.1  # Add 0.1 for positive sentiment\n",
    "    elif doc.vocab.strings[match_id] == \"SAD\":\n",
    "        doc.sentiment -= 0.1  # Subtract 0.1 for negative sentiment\n",
    "\n",
    "matcher.add(\"HAPPY\", pos_patterns, on_match=label_sentiment)  # Add positive pattern\n",
    "matcher.add(\"SAD\", neg_patterns, on_match=label_sentiment)  # Add negative pattern\n",
    "\n",
    "# Add pattern for valid hashtag, i.e. '#' plus any ASCII token\n",
    "matcher.add(\"HASHTAG\", [[{\"ORTH\": \"#\"}, {\"IS_ASCII\": True}]])\n",
    "\n",
    "doc = nlp(\"Hello world 😀 #MondayMotivation\")\n",
    "#doc1= nlp ('Ciudadana, senadora de Colombia 🇨🇴 Partido Verde 🌻 Activista de las causas en que creo #ElCambioEsImparable! #MujeresBerracas💪#ColombiaSeRespeta!')\n",
    "matches = matcher(doc1)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = doc1.vocab.strings[match_id]  # Look up string ID\n",
    "    span = doc1[start:end]\n",
    "    print(string_id, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-animation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
