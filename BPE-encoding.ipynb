{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/deep-learning/anaconda3/lib/python3.7/site-packages (0.1.85)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[699, 31, 46, 984, 5, 71, 34, 211, 39]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.Train('--input=labiblia.txt --model_prefix=m --vocab_size=1000')\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(\"m.model\")\n",
    "sp.EncodeAsIds(\"Este es un ejemplo de una novela\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‚ñÅEste', '‚ñÅes', '‚ñÅel', '‚ñÅejemplo', '‚ñÅde', '‚ñÅuna', '‚ñÅno', 've', 'la', '‚ñÅco', 'lo', 'm', 'bi', 'a', 'na']\n",
      "[699, 31, 13, 984, 5, 71, 34, 211, 39, 245, 63, 79, 139, 19, 67]\n"
     ]
    }
   ],
   "source": [
    "print(sp.encode_as_pieces('Este es el ejemplo de una novela colombiana'))\n",
    "print(sp.encode_as_ids('Este es el ejemplo de una novela colombiana'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE con SentencePiece\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentencePiece soporta el modelo BPE para segmentaci√≥n de subpalabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=labiblia.txt --model_prefix=m_bpe --vocab_size=2000 --model_type=bpe')\n",
    "sp_bpe = spm.SentencePieceProcessor()\n",
    "sp_bpe.load('m_bpe.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso se comrpueba que los ids de los dos modelos el de palabra y el bpe son diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** BPE ***\n",
      "['‚ñÅEste', '‚ñÅes', '‚ñÅel', '‚ñÅlib', 'ri', 'to', '‚ñÅde', '‚ñÅJob']\n",
      "[1340, 53, 29, 366, 51, 66, 7, 1520]\n"
     ]
    }
   ],
   "source": [
    "print('*** BPE ***')\n",
    "print(sp_bpe.encode_as_pieces('Este es el librito de Job'))\n",
    "print(sp_bpe.encode_as_ids('Este es el librito de Job'))\n",
    "#print(sp_word.encode_as_pieces('Este es el librito de Job'))\n",
    "#print(sp_word.encode_as_ids('Este es el librito de Job'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentaci√≥n de palabras con SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‚ñÅEste', '‚ñÅes', '‚ñÅel', '‚ñÅlibrito', '‚ñÅde', '‚ñÅJob.']\n",
      "[298, 21, 6, 219, 3, 0]\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=labiblia.txt --model_prefix=m_word --model_type=word --vocab_size=2000')\n",
    "\n",
    "sp_word = spm.SentencePieceProcessor()\n",
    "sp_word.load('m_word.model')\n",
    "\n",
    "print(sp_word.encode_as_pieces('Este es el librito de  Job.'))  # '.' will not be one token.\n",
    "print(sp_word.encode_as_ids('Este es el libro de Job.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de prefijos  con SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‚ñÅEste', '‚ñÅes', '‚ñÅel', '‚ñÅli', 'b', 'ri', 'to', '‚ñÅde', '‚ñÅJob']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sp_word' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3353b8693aa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'm.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_as_pieces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Este es el librito de Job'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_as_pieces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'¬øQuieres saber c√≥mo hacer una aplicaci√≥n para Android Wear? Te lo explicamos &gt; http://t.co/IjgCIkigvm http://t.co/tUZhU3wrhx '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_as_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Este es el librito de Job'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sp_word' is not defined"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.train('--input=labiblia.txt --model_prefix=m --vocab_size=2000')\n",
    "\n",
    "# makes segmenter instance and loads the model file (m.model)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n",
    "print(sp.encode_as_pieces('Este es el librito de Job'))\n",
    "print(sp_word.encode_as_pieces('¬øQuieres saber c√≥mo hacer una aplicaci√≥n para Android Wear? Te lo explicamos &gt; http://t.co/IjgCIkigvm http://t.co/tUZhU3wrhx '))\n",
    "print(sp.encode_as_ids('Este es el librito de Job'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo caracter con SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‚ñÅ', 'E', 's', 't', 'e', '‚ñÅ', 'e', 's', '‚ñÅ', 'e', 'l', '‚ñÅ', 'l', 'i', 'b', 'r', 'i', 't', 'o', '‚ñÅ', 'd', 'e', '‚ñÅ', 'J', 'o', 'b', '.']\n",
      "[3, 36, 7, 13, 4, 3, 4, 7, 3, 4, 11, 3, 11, 10, 19, 8, 10, 13, 6, 3, 12, 4, 3, 43, 6, 19, 20]\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=labiblia.txt --model_prefix=m_char --model_type=char --vocab_size=400')\n",
    "\n",
    "sp_char = spm.SentencePieceProcessor()\n",
    "sp_char.load('m_char.model')\n",
    "\n",
    "print(sp_char.encode_as_pieces('Este es el librito de Job.'))\n",
    "print(sp_char.encode_as_ids('Este es el librito de Job.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de usuario deifindo para usar con BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‚ñÅ', 't', 'hi', 's', '<sep>', '‚ñÅ', 'is', '‚ñÅa', '‚ñÅte', 's', 't', '‚ñÅhe', 'llo', '‚ñÅ', 'w', 'or', 'l', 'd', '<cls>']\n",
      "3\n",
      "4\n",
      "3= <sep>\n",
      "4= <cls>\n",
      "['‚ñÅ', '<', 's', '>', 'T', 'w', '3', '3', 't', '<sep>', '‚ñÅ#', '‚ñÅ@', 'du', 'de', '_', 'rea', 'll', 'y', '<', 'un', 'k', '>', '‚ñÅ#', 'ha', 's', 'h', '_', 't', 'a', 'g', '‚ñÅ', '$', 'hi', 't', '‚ñÅ(', 'g', '@', 'y', ')', '‚ñÅre', 't', 'ar', 'd', '#', 'd', '‚ñÅ@', 'du', 'de', '.', '‚ñÅ', 'üòÄüòÄ', '‚ñÅ', '!', 'üòÄ', 'a', 'b', 'c', '‚ñÅ', '%', 'üòÄ', 'lo', 'l', '‚ñÅ#', 'ha', 'te', 'i', 't', '‚ñÅ#', 'ha', 'te', '.', 'i', 't', '‚ñÅ', '$', '%', '&', '/', '‚ñÅf', '*', 'ck', '-', '‚ñÅin', '‚ñÅlo', 've', '_', 't', 'wi', 't', 't', 'er', '<cls>', '<', '/', 's', '>']\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=tweets_clean.txt --model_prefix=m_user --user_defined_symbols=<sep>,<cls> --vocab_size=2000')\n",
    "# ids are reserved in both mode.\n",
    "# <unk>=0, <s>=1, </s>=2, <sep>=3, <cls>=4\n",
    "# user defined symbols allow these symbol to apper in the text.\n",
    "sp_user = spm.SentencePieceProcessor()\n",
    "sp_user.load('m_user.model')\n",
    "print(sp_user.encode_as_pieces('this<sep> is a test hello world<cls>'))\n",
    "print(sp_user.piece_to_id('<sep>'))  # 3\n",
    "print(sp_user.piece_to_id('<cls>'))  # 4\n",
    "print('3=', sp_user.decode_ids([3]))  # decoded to <sep>\n",
    "print('4=', sp_user.decode_ids([4]))  # decoded to <cls>\n",
    "print(sp_user.encode_as_pieces('<s>Tw33t<sep> # @dude_really<unk> #hash_tag $hit (g@y) retard#d @dude. üòÄüòÄ !üòÄabc %üòÄlol #hateit #hate.it $%&/ f*ck- in love_twitter<cls></s>'))\n",
    "#print('0=', sp_user.decode_ids([0]))  # decoded to <cls>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‚ñÅ¬øQuieres<sep>', '‚ñÅsaber', '‚ñÅc√≥mo', '‚ñÅhacer', '‚ñÅuna', '‚ñÅaplicaci√≥n', '‚ñÅpara', '‚ñÅAndroid', '‚ñÅWear?', '‚ñÅTe', '‚ñÅlo', '‚ñÅexplicamos', '‚ñÅ&gt;', '‚ñÅhttp://t.co/IjgCIkigvm‚ñÅhttp://t.co/tUZhU3wrhx']\n",
      "[102, 15, 7, 785, 3, 0]\n",
      "['‚ñÅ@HyperKynetic', '‚ñÅsi', '‚ñÅme', '‚ñÅlos', '‚ñÅtraes', '‚ñÅa', '‚ñÅla', '‚ñÅte', '‚ñÅquerr√≠a', '‚ñÅpara', '‚ñÅsiempre', '‚ñÅüòÅ', '‚ñÅque', '‚ñÅen', '‚ñÅ#ayunas', '‚ñÅpor', '‚ñÅsacarme', '‚ñÅsangre', '‚ñÅ:(']\n",
      "['‚ñÅTw33t‚ñÅ#‚ñÅ@dude_really‚ñÅ#hash_tag‚ñÅ$hit‚ñÅ(g@y)‚ñÅretard#d‚ñÅ@dude.‚ñÅüòÄüòÄ‚ñÅ!üòÄabc‚ñÅ%üòÄlol‚ñÅ#hateit‚ñÅ#hate.it‚ñÅ$%&/‚ñÅf*ck-', '‚ñÅin', '‚ñÅlove_twitter']\n",
      "['‚ñÅCigarette‚ñÅsmoke‚ñÅcontains‚ñÅpoisonous‚ñÅelements‚ñÅthat‚ñÅgo‚ñÅinto‚ñÅour‚ñÅbody', '‚ñÅand', '‚ñÅmake‚ñÅus‚ñÅsuffer‚ñÅfrom‚ñÅit,due', '‚ñÅto', '‚ñÅwhich', '‚ñÅa', '‚ñÅdeadly‚ñÅdisease‚ñÅlike‚ñÅcancer', '‚ñÅis', '‚ñÅborn.‚ñÅStop‚ñÅsmoking‚ñÅtobacco!‚ñÅPlease‚ñÅdo‚ñÅlisten', '‚ñÅto', '‚ñÅsatsang‚ñÅdaily‚ñÅon‚ñÅ#GodMorningFriday#,‚ñÅ#Coronvirus']\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.train('--input=tweets_clean.txt --model_prefix=m_word --model_type=word --vocab_size=2000')\n",
    "\n",
    "sp_word = spm.SentencePieceProcessor()\n",
    "sp_word.load('m_word.model')\n",
    "\n",
    "print(sp_word.encode_as_pieces('¬øQuieres<sep> saber c√≥mo hacer una aplicaci√≥n para Android Wear? Te lo explicamos &gt; http://t.co/IjgCIkigvm http://t.co/tUZhU3wrhx '))\n",
    "print(sp_word.encode_as_ids('Este es el libro de Job.'))\n",
    "print(sp_word.encode_as_pieces(\"@HyperKynetic si me los traes a la te querr√≠a para siempre üòÅ que  en #ayunas por sacarme sangre :( \"))\n",
    "print(sp_word.encode_as_pieces('Tw33t # @dude_really #hash_tag $hit (g@y) retard#d @dude. üòÄüòÄ !üòÄabc %üòÄlol #hateit #hate.it $%&/ f*ck- in love_twitter'))\n",
    "print(sp_word.encode_as_pieces('Cigarette smoke contains poisonous elements that go into our body and make us suffer from it,due to which a deadly disease like cancer is born. Stop smoking tobacco! Please do listen to satsang daily on #GodMorningFriday#, #Coronvirus'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
