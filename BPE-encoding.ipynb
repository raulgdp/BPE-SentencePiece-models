{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.2 MB 852 kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.95\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo b√°sico de prefijos y decodificadores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[686, 29, 47, 981, 5, 70, 32, 233, 41]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.Train('--input=labiblia.txt --model_prefix=m --vocab_size=1000')\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(\"m.model\")\n",
    "sp.EncodeAsIds(\"Este es un ejemplo de una novela\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‚ñÅEste', '‚ñÅes', '‚ñÅel', '‚ñÅejemplo', '‚ñÅde', '‚ñÅuna', '‚ñÅno', 've', 'la', '‚ñÅco', 'lo', 'm', 'bi', 'an', 'a']\n",
      "[686, 29, 13, 981, 5, 70, 32, 233, 41, 143, 89, 48, 135, 124, 15]\n",
      "Este es el ejemplo de una novela colombiana\n"
     ]
    }
   ],
   "source": [
    "print(sp.encode_as_pieces('Este es el ejemplo de una novela colombiana'))\n",
    "print(sp.encode_as_ids('Este es el ejemplo de una novela colombiana'))\n",
    "print(sp.decode_ids([686, 29, 13, 981, 5, 70, 32, 233, 41, 143, 89, 48, 135, 124, 15]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentencePiece con el modelo bpe real\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentencePiece soporta el modelo original de BPE para segmentaci√≥n de subpalabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=labiblia.txt --model_prefix=m_bpe --vocab_size=2000 --model_type=bpe')\n",
    "sp_bpe = spm.SentencePieceProcessor()\n",
    "sp_bpe.load('m_bpe.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso se comrpueba que los ids de los dos modelos el de palabra y el bpe son diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** BPE ***\n",
      "['‚ñÅEste', '‚ñÅes', '‚ñÅel', '‚ñÅlib', 'ri', 'to', '‚ñÅde', '‚ñÅJob']\n",
      "[1340, 53, 29, 366, 51, 66, 7, 1520]\n"
     ]
    }
   ],
   "source": [
    "print('*** BPE ***')\n",
    "print(sp_bpe.encode_as_pieces('Este es el librito de Job'))\n",
    "print(sp_bpe.encode_as_ids('Este es el librito de Job'))\n",
    "#print(sp_word.encode_as_pieces('Este es el librito de Job'))\n",
    "#print(sp_word.encode_as_ids('Este es el librito de Job'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentaci√≥n de palabras con SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‚ñÅTodos', '‚ñÅlos', '‚ñÅprofesores,‚ñÅestudian', '‚ñÅpara', '‚ñÅPh.D', '‚ñÅy', '‚ñÅtodos', '‚ñÅme', '‚ñÅodian']\n",
      "[1094, 11, 0, 26, 0, 5, 102, 22, 0]\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=hojarasca1.txt --model_prefix=m_word --model_type=word --vocab_size=5000')\n",
    "\n",
    "sp_word = spm.SentencePieceProcessor()\n",
    "sp_word.load('m_word.model')\n",
    "\n",
    "#print(sp_word.encode_as_pieces('Este es el librito de  Job.'))  # '.' will not be one token.\n",
    "#print(sp_word.encode_as_ids('Este es el libro de Job.'))\n",
    "print(sp_word.encode_as_pieces('Todos los profesores, estudian para Ph.D y todos me odian'))\n",
    "print(sp_word.encode_as_ids('Todos los profesores, estudian para Ph.D y todos me odian')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Que puedo haber pasado con _profesores_estudian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de prefijos  con SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‚ñÅEste', '‚ñÅes', '‚ñÅel', '‚ñÅli', 'b', 'ri', 'to', '‚ñÅde', '‚ñÅJob']\n",
      "[681, 30, 10, 432, 44, 93, 59, 5, 995]\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.train('--input=labiblia.txt --model_prefix=m --vocab_size=2000')\n",
    "\n",
    "# makes segmenter instance and loads the model file (m.model)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n",
    "print(sp.encode_as_pieces('Este es el librito de Job'))\n",
    "#print(sp_word.encode_as_pieces('¬øQuieres saber c√≥mo hacer una aplicaci√≥n para Android Wear? Te lo explicamos &gt; http://t.co/IjgCIkigvm http://t.co/tUZhU3wrhx '))\n",
    "print(sp.encode_as_ids('Este es el librito de Job'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo caracter con SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‚ñÅ', 'E', 's', 't', 'e', '‚ñÅ', 'e', 's', '‚ñÅ', 'e', 'l', '‚ñÅ', 'l', 'i', 'b', 'r', 'i', 't', 'o', '‚ñÅ', 'd', 'e', '‚ñÅ', 'J', 'o', 'b', '.']\n",
      "[3, 36, 7, 13, 4, 3, 4, 7, 3, 4, 11, 3, 11, 10, 19, 8, 10, 13, 6, 3, 12, 4, 3, 43, 6, 19, 20]\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=labiblia.txt --model_prefix=m_char --model_type=char --vocab_size=400')\n",
    "\n",
    "sp_char = spm.SentencePieceProcessor()\n",
    "sp_char.load('m_char.model')\n",
    "\n",
    "print(sp_char.encode_as_pieces('Este es el librito de Job.'))\n",
    "print(sp_char.encode_as_ids('Este es el librito de Job.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de usuario defindo para usar con BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‚ñÅ', 't', 'hi', 's', '<sep>', '‚ñÅ', 'is', '‚ñÅa', '‚ñÅte', 's', 't', '‚ñÅhe', 'llo', '‚ñÅ', 'w', 'or', 'l', 'd', '<cls>']\n",
      "3\n",
      "4\n",
      "3= <sep>\n",
      "4= <cls>\n",
      "['‚ñÅ', '<', 's', '>', 'T', 'w', '3', '3', 't', '<sep>', '‚ñÅ#', '‚ñÅ@', 'du', 'de', '_', 'rea', 'll', 'y', '<', 'un', 'k', '>', '‚ñÅ#', 'ha', 's', 'h', '_', 't', 'a', 'g', '‚ñÅ', '$', 'hi', 't', '‚ñÅ(', 'g', '@', 'y', ')', '‚ñÅre', 't', 'ar', 'd', '#', 'd', '‚ñÅ@', 'du', 'de', '.', '‚ñÅ', 'üòÄüòÄ', '‚ñÅ', '!', 'üòÄ', 'a', 'b', 'c', '‚ñÅ', '%', 'üòÄ', 'lo', 'l', '‚ñÅ#', 'ha', 'te', 'i', 't', '‚ñÅ#', 'ha', 'te', '.', 'i', 't', '‚ñÅ', '$', '%', '&', '/', '‚ñÅf', '*', 'ck', '-', '‚ñÅin', '‚ñÅlo', 've', '_', 't', 'wi', 't', 't', 'er', '<cls>', '<', '/', 's', '>']\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=tweets_clean.txt --model_prefix=m_user --user_defined_symbols=<sep>,<cls> --vocab_size=2000')\n",
    "# ids are reserved in both mode.\n",
    "# <unk>=0, <s>=1, </s>=2, <sep>=3, <cls>=4\n",
    "# user defined symbols allow these symbol to apper in the text.\n",
    "sp_user = spm.SentencePieceProcessor()\n",
    "sp_user.load('m_user.model')\n",
    "print(sp_user.encode_as_pieces('this<sep> is a test hello world<cls>'))\n",
    "print(sp_user.piece_to_id('<sep>'))  # 3\n",
    "print(sp_user.piece_to_id('<cls>'))  # 4\n",
    "print('3=', sp_user.decode_ids([3]))  # decoded to <sep>\n",
    "print('4=', sp_user.decode_ids([4]))  # decoded to <cls>\n",
    "print(sp_user.encode_as_pieces('<s>Tw33t<sep> # @dude_really<unk> #hash_tag $hit (g@y) retard#d @dude. üòÄüòÄ !üòÄabc %üòÄlol #hateit #hate.it $%&/ f*ck- in love_twitter<cls></s>'))\n",
    "#print('0=', sp_user.decode_ids([0]))  # decoded to <cls>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‚ñÅ', '<', 's', '>', '‚ñÅTodo', 's', '‚ñÅlos', '‚ñÅco', 'l', 'm', 'bi', 'a', 'nos', '‚ñÅca', 'nta', 'n', '</', 's', '>']\n",
      "['‚ñÅ', '<s>', '‚ñÅTodo', 's', '‚ñÅlo', 's', '‚ñÅc', 'olombia', 'n', 'os', '‚ñÅcant', 'an', '</s>']\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=hojarasca1.txt --model_prefix=m_bos_as_user --user_defined_symbols=<s>,</s> --vocab_size=2000')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n",
    "print(sp.encode_as_pieces('<s> Todos los colmbianos cantan</s>'))   # <s>,</s> are segmented. (default behavior)\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m_bos_as_user.model')\n",
    "print(sp.encode_as_pieces('<s> Todos los colombianos cantan</s>'))   # <s>,</s> are handled as one token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Este entrena   un modelo de palabra con tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‚ñÅ¬øQuieres<sep>', '‚ñÅsaber', '‚ñÅc√≥mo', '‚ñÅhacer', '‚ñÅuna', '‚ñÅaplicaci√≥n', '‚ñÅpara', '‚ñÅAndroid', '‚ñÅWear?', '‚ñÅTe', '‚ñÅlo', '‚ñÅexplicamos', '‚ñÅ&gt;', '‚ñÅhttp://t.co/IjgCIkigvm‚ñÅhttp://t.co/tUZhU3wrhx']\n",
      "[102, 15, 7, 785, 3, 0]\n",
      "['‚ñÅ@HyperKynetic', '‚ñÅsi', '‚ñÅme', '‚ñÅlos', '‚ñÅtraes', '‚ñÅa', '‚ñÅla', '‚ñÅte', '‚ñÅquerr√≠a', '‚ñÅpara', '‚ñÅsiempre', '‚ñÅüòÅ', '‚ñÅque', '‚ñÅen', '‚ñÅ#ayunas', '‚ñÅpor', '‚ñÅsacarme', '‚ñÅsangre', '‚ñÅ:(']\n",
      "['‚ñÅTw33t‚ñÅ#‚ñÅ@dude_really‚ñÅ#hash_tag‚ñÅ$hit‚ñÅ(g@y)‚ñÅretard#d‚ñÅ@dude.‚ñÅüòÄüòÄ‚ñÅ!üòÄabc‚ñÅ%üòÄlol‚ñÅ#hateit‚ñÅ#hate.it‚ñÅ$%&/‚ñÅf*ck-', '‚ñÅin', '‚ñÅlove_twitter']\n",
      "['‚ñÅCigarette‚ñÅsmoke‚ñÅcontains‚ñÅpoisonous‚ñÅelements‚ñÅthat‚ñÅgo‚ñÅinto‚ñÅour‚ñÅbody', '‚ñÅand', '‚ñÅmake‚ñÅus‚ñÅsuffer‚ñÅfrom‚ñÅit,due', '‚ñÅto', '‚ñÅwhich', '‚ñÅa', '‚ñÅdeadly‚ñÅdisease‚ñÅlike‚ñÅcancer', '‚ñÅis', '‚ñÅborn.‚ñÅStop‚ñÅsmoking‚ñÅtobacco!‚ñÅPlease‚ñÅdo‚ñÅlisten', '‚ñÅto', '‚ñÅsatsang‚ñÅdaily‚ñÅon‚ñÅ#GodMorningFriday#,‚ñÅ#Coronvirus']\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.train('--input=tweets_clean.txt --model_prefix=m_word --model_type=word --vocab_size=2000')\n",
    "\n",
    "sp_word = spm.SentencePieceProcessor()\n",
    "sp_word.load('m_word.model')\n",
    "\n",
    "print(sp_word.encode_as_pieces('¬øQuieres<sep> saber c√≥mo hacer una aplicaci√≥n para Android Wear? Te lo explicamos &gt; http://t.co/IjgCIkigvm http://t.co/tUZhU3wrhx '))\n",
    "print(sp_word.encode_as_ids('Este es el libro de Job.'))\n",
    "print(sp_word.encode_as_pieces(\"@HyperKynetic si me los traes a la te querr√≠a para siempre üòÅ que  en #ayunas por sacarme sangre :( \"))\n",
    "print(sp_word.encode_as_pieces('Tw33t # @dude_really #hash_tag $hit (g@y) retard#d @dude. üòÄüòÄ !üòÄabc %üòÄlol #hateit #hate.it $%&/ f*ck- in love_twitter'))\n",
    "print(sp_word.encode_as_pieces('Cigarette smoke contains poisonous elements that go into our body and make us suffer from it,due to which a deadly disease like cancer is born. Stop smoking tobacco! Please do listen to satsang daily on #GodMorningFriday#, #Coronvirus'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeo de Wordpiece sobre BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    TF2_WEIGHTS_NAME,\n",
    "    BertConfig,\n",
    "    BertTokenizer,\n",
    "    TFBertForTokenClassification,\n",
    "    create_optimizer)\n",
    "\n",
    "configuration = BertConfig()\n",
    "BERT_MODEL = \"bert-base-multilingual-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Todos', 'los', 'profesores', 'estudi', '##an', 'para', 'Ph', '.', 'D', 'y', 'todos', 'me', 'od', '##ian']\n"
     ]
    }
   ],
   "source": [
    "MODEL_CLASSES = {\"bert\": (BertConfig, TFBertForTokenClassification, BertTokenizer)}\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES['bert']\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(BERT_MODEL, do_lower_case=False)\n",
    "\n",
    "word = \"Todos los profesores estudian para Ph.D y todos me odian\"\n",
    "\n",
    "word_tokens = tokenizer.tokenize(str(word))\n",
    "\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
