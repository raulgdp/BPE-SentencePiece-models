{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.2 MB 852 kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.95\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo bÃ¡sico de prefijos y decodificadores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[686, 29, 47, 981, 5, 70, 32, 233, 41]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.Train('--input=labiblia.txt --model_prefix=m --vocab_size=1000')\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(\"m.model\")\n",
    "sp.EncodeAsIds(\"Este es un ejemplo de una novela\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–Este', 'â–es', 'â–el', 'â–ejemplo', 'â–de', 'â–una', 'â–no', 've', 'la', 'â–co', 'lo', 'm', 'bi', 'an', 'a']\n",
      "[686, 29, 13, 981, 5, 70, 32, 233, 41, 143, 89, 48, 135, 124, 15]\n",
      "Este es el ejemplo de una novela colombiana\n"
     ]
    }
   ],
   "source": [
    "print(sp.encode_as_pieces('Este es el ejemplo de una novela colombiana'))\n",
    "print(sp.encode_as_ids('Este es el ejemplo de una novela colombiana'))\n",
    "print(sp.decode_ids([686, 29, 13, 981, 5, 70, 32, 233, 41, 143, 89, 48, 135, 124, 15]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentencePiece con el modelo bpe real\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentencePiece soporta el modelo original de BPE para segmentaciÃ³n de subpalabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=labiblia.txt --model_prefix=m_bpe --vocab_size=2000 --model_type=bpe')\n",
    "sp_bpe = spm.SentencePieceProcessor()\n",
    "sp_bpe.load('m_bpe.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso se comrpueba que los ids de los dos modelos el de palabra y el bpe son diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** BPE ***\n",
      "['â–Este', 'â–es', 'â–el', 'â–lib', 'ri', 'to', 'â–de', 'â–Job']\n",
      "[1340, 53, 29, 366, 51, 66, 7, 1520]\n"
     ]
    }
   ],
   "source": [
    "print('*** BPE ***')\n",
    "print(sp_bpe.encode_as_pieces('Este es el librito de Job'))\n",
    "print(sp_bpe.encode_as_ids('Este es el librito de Job'))\n",
    "#print(sp_word.encode_as_pieces('Este es el librito de Job'))\n",
    "#print(sp_word.encode_as_ids('Este es el librito de Job'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SegmentaciÃ³n de palabras con SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–Todos', 'â–los', 'â–profesores,â–estudian', 'â–para', 'â–Ph.D', 'â–y', 'â–todos', 'â–me', 'â–odian']\n",
      "[1094, 11, 0, 26, 0, 5, 102, 22, 0]\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=hojarasca1.txt --model_prefix=m_word --model_type=word --vocab_size=5000')\n",
    "\n",
    "sp_word = spm.SentencePieceProcessor()\n",
    "sp_word.load('m_word.model')\n",
    "\n",
    "#print(sp_word.encode_as_pieces('Este es el librito de  Job.'))  # '.' will not be one token.\n",
    "#print(sp_word.encode_as_ids('Este es el libro de Job.'))\n",
    "print(sp_word.encode_as_pieces('Todos los profesores, estudian para Ph.D y todos me odian'))\n",
    "print(sp_word.encode_as_ids('Todos los profesores, estudian para Ph.D y todos me odian')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Que puedo haber pasado con _profesores_estudian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de prefijos  con SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–Este', 'â–es', 'â–el', 'â–li', 'b', 'ri', 'to', 'â–de', 'â–Job']\n",
      "[681, 30, 10, 432, 44, 93, 59, 5, 995]\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.train('--input=labiblia.txt --model_prefix=m --vocab_size=2000')\n",
    "\n",
    "# makes segmenter instance and loads the model file (m.model)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n",
    "print(sp.encode_as_pieces('Este es el librito de Job'))\n",
    "#print(sp_word.encode_as_pieces('Â¿Quieres saber cÃ³mo hacer una aplicaciÃ³n para Android Wear? Te lo explicamos &gt; http://t.co/IjgCIkigvm http://t.co/tUZhU3wrhx '))\n",
    "print(sp.encode_as_ids('Este es el librito de Job'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo caracter con SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–', 'E', 's', 't', 'e', 'â–', 'e', 's', 'â–', 'e', 'l', 'â–', 'l', 'i', 'b', 'r', 'i', 't', 'o', 'â–', 'd', 'e', 'â–', 'J', 'o', 'b', '.']\n",
      "[3, 36, 7, 13, 4, 3, 4, 7, 3, 4, 11, 3, 11, 10, 19, 8, 10, 13, 6, 3, 12, 4, 3, 43, 6, 19, 20]\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=labiblia.txt --model_prefix=m_char --model_type=char --vocab_size=400')\n",
    "\n",
    "sp_char = spm.SentencePieceProcessor()\n",
    "sp_char.load('m_char.model')\n",
    "\n",
    "print(sp_char.encode_as_pieces('Este es el librito de Job.'))\n",
    "print(sp_char.encode_as_ids('Este es el librito de Job.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de usuario defindo para usar con BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–', 't', 'hi', 's', '<sep>', 'â–', 'is', 'â–a', 'â–te', 's', 't', 'â–he', 'llo', 'â–', 'w', 'or', 'l', 'd', '<cls>']\n",
      "3\n",
      "4\n",
      "3= <sep>\n",
      "4= <cls>\n",
      "['â–', '<', 's', '>', 'T', 'w', '3', '3', 't', '<sep>', 'â–#', 'â–@', 'du', 'de', '_', 'rea', 'll', 'y', '<', 'un', 'k', '>', 'â–#', 'ha', 's', 'h', '_', 't', 'a', 'g', 'â–', '$', 'hi', 't', 'â–(', 'g', '@', 'y', ')', 'â–re', 't', 'ar', 'd', '#', 'd', 'â–@', 'du', 'de', '.', 'â–', 'ğŸ˜€ğŸ˜€', 'â–', '!', 'ğŸ˜€', 'a', 'b', 'c', 'â–', '%', 'ğŸ˜€', 'lo', 'l', 'â–#', 'ha', 'te', 'i', 't', 'â–#', 'ha', 'te', '.', 'i', 't', 'â–', '$', '%', '&', '/', 'â–f', '*', 'ck', '-', 'â–in', 'â–lo', 've', '_', 't', 'wi', 't', 't', 'er', '<cls>', '<', '/', 's', '>']\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=tweets_clean.txt --model_prefix=m_user --user_defined_symbols=<sep>,<cls> --vocab_size=2000')\n",
    "# ids are reserved in both mode.\n",
    "# <unk>=0, <s>=1, </s>=2, <sep>=3, <cls>=4\n",
    "# user defined symbols allow these symbol to apper in the text.\n",
    "sp_user = spm.SentencePieceProcessor()\n",
    "sp_user.load('m_user.model')\n",
    "print(sp_user.encode_as_pieces('this<sep> is a test hello world<cls>'))\n",
    "print(sp_user.piece_to_id('<sep>'))  # 3\n",
    "print(sp_user.piece_to_id('<cls>'))  # 4\n",
    "print('3=', sp_user.decode_ids([3]))  # decoded to <sep>\n",
    "print('4=', sp_user.decode_ids([4]))  # decoded to <cls>\n",
    "print(sp_user.encode_as_pieces('<s>Tw33t<sep> # @dude_really<unk> #hash_tag $hit (g@y) retard#d @dude. ğŸ˜€ğŸ˜€ !ğŸ˜€abc %ğŸ˜€lol #hateit #hate.it $%&/ f*ck- in love_twitter<cls></s>'))\n",
    "#print('0=', sp_user.decode_ids([0]))  # decoded to <cls>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–', '<', 's', '>', 'â–Todo', 's', 'â–los', 'â–co', 'l', 'm', 'bi', 'a', 'nos', 'â–ca', 'nta', 'n', '</', 's', '>']\n",
      "['â–', '<s>', 'â–Todo', 's', 'â–lo', 's', 'â–c', 'olombia', 'n', 'os', 'â–cant', 'an', '</s>']\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=hojarasca1.txt --model_prefix=m_bos_as_user --user_defined_symbols=<s>,</s> --vocab_size=2000')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n",
    "print(sp.encode_as_pieces('<s> Todos los colmbianos cantan</s>'))   # <s>,</s> are segmented. (default behavior)\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m_bos_as_user.model')\n",
    "print(sp.encode_as_pieces('<s> Todos los colombianos cantan</s>'))   # <s>,</s> are handled as one token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Este entrena   un modelo de palabra con tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–Â¿Quieres<sep>', 'â–saber', 'â–cÃ³mo', 'â–hacer', 'â–una', 'â–aplicaciÃ³n', 'â–para', 'â–Android', 'â–Wear?', 'â–Te', 'â–lo', 'â–explicamos', 'â–&gt;', 'â–http://t.co/IjgCIkigvmâ–http://t.co/tUZhU3wrhx']\n",
      "[102, 15, 7, 785, 3, 0]\n",
      "['â–@HyperKynetic', 'â–si', 'â–me', 'â–los', 'â–traes', 'â–a', 'â–la', 'â–te', 'â–querrÃ­a', 'â–para', 'â–siempre', 'â–ğŸ˜', 'â–que', 'â–en', 'â–#ayunas', 'â–por', 'â–sacarme', 'â–sangre', 'â–:(']\n",
      "['â–Tw33tâ–#â–@dude_reallyâ–#hash_tagâ–$hitâ–(g@y)â–retard#dâ–@dude.â–ğŸ˜€ğŸ˜€â–!ğŸ˜€abcâ–%ğŸ˜€lolâ–#hateitâ–#hate.itâ–$%&/â–f*ck-', 'â–in', 'â–love_twitter']\n",
      "['â–Cigaretteâ–smokeâ–containsâ–poisonousâ–elementsâ–thatâ–goâ–intoâ–ourâ–body', 'â–and', 'â–makeâ–usâ–sufferâ–fromâ–it,due', 'â–to', 'â–which', 'â–a', 'â–deadlyâ–diseaseâ–likeâ–cancer', 'â–is', 'â–born.â–Stopâ–smokingâ–tobacco!â–Pleaseâ–doâ–listen', 'â–to', 'â–satsangâ–dailyâ–onâ–#GodMorningFriday#,â–#Coronvirus']\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.train('--input=tweets_clean.txt --model_prefix=m_word --model_type=word --vocab_size=2000')\n",
    "\n",
    "sp_word = spm.SentencePieceProcessor()\n",
    "sp_word.load('m_word.model')\n",
    "\n",
    "print(sp_word.encode_as_pieces('Â¿Quieres<sep> saber cÃ³mo hacer una aplicaciÃ³n para Android Wear? Te lo explicamos &gt; http://t.co/IjgCIkigvm http://t.co/tUZhU3wrhx '))\n",
    "print(sp_word.encode_as_ids('Este es el libro de Job.'))\n",
    "print(sp_word.encode_as_pieces(\"@HyperKynetic si me los traes a la te querrÃ­a para siempre ğŸ˜ que  en #ayunas por sacarme sangre :( \"))\n",
    "print(sp_word.encode_as_pieces('Tw33t # @dude_really #hash_tag $hit (g@y) retard#d @dude. ğŸ˜€ğŸ˜€ !ğŸ˜€abc %ğŸ˜€lol #hateit #hate.it $%&/ f*ck- in love_twitter'))\n",
    "print(sp_word.encode_as_pieces('Cigarette smoke contains poisonous elements that go into our body and make us suffer from it,due to which a deadly disease like cancer is born. Stop smoking tobacco! Please do listen to satsang daily on #GodMorningFriday#, #Coronvirus'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeo de Wordpiece sobre BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    TF2_WEIGHTS_NAME,\n",
    "    BertConfig,\n",
    "    BertTokenizer,\n",
    "    TFBertForTokenClassification,\n",
    "    create_optimizer)\n",
    "\n",
    "configuration = BertConfig()\n",
    "BERT_MODEL = \"bert-base-multilingual-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Todos', 'los', 'profesores', 'estudi', '##an', 'para', 'Ph', '.', 'D', 'y', 'todos', 'me', 'od', '##ian']\n"
     ]
    }
   ],
   "source": [
    "MODEL_CLASSES = {\"bert\": (BertConfig, TFBertForTokenClassification, BertTokenizer)}\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES['bert']\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(BERT_MODEL, do_lower_case=False)\n",
    "\n",
    "word = \"Todos los profesores estudian para Ph.D y todos me odian\"\n",
    "\n",
    "word_tokens = tokenizer.tokenize(str(word))\n",
    "\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
