{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/deep-learning/anaconda3/lib/python3.7/site-packages (0.1.85)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[699, 31, 46, 984, 5, 71, 34, 211, 39]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.Train('--input=labiblia.txt --model_prefix=m --vocab_size=1000')\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(\"m.model\")\n",
    "sp.EncodeAsIds(\"Este es un ejemplo de una novela\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–Este', 'â–es', 'â–el', 'â–ejemplo', 'â–de', 'â–una', 'â–no', 've', 'la', 'â–co', 'lo', 'm', 'bi', 'a', 'na']\n",
      "[699, 31, 13, 984, 5, 71, 34, 211, 39, 245, 63, 79, 139, 19, 67]\n"
     ]
    }
   ],
   "source": [
    "print(sp.encode_as_pieces('Este es el ejemplo de una novela colombiana'))\n",
    "print(sp.encode_as_ids('Este es el ejemplo de una novela colombiana'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE con SentencePiece\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentencePiece soporta el modelo BPE para segmentaciÃ³n de subpalabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=labiblia.txt --model_prefix=m_bpe --vocab_size=2000 --model_type=bpe')\n",
    "sp_bpe = spm.SentencePieceProcessor()\n",
    "sp_bpe.load('m_bpe.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso se comrpueba que los ids de los dos modelos el de palabra y el bpe son diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** BPE ***\n",
      "['â–Este', 'â–es', 'â–el', 'â–lib', 'ri', 'to', 'â–de', 'â–Job']\n",
      "[1340, 53, 29, 366, 51, 66, 7, 1520]\n"
     ]
    }
   ],
   "source": [
    "print('*** BPE ***')\n",
    "print(sp_bpe.encode_as_pieces('Este es el librito de Job'))\n",
    "print(sp_bpe.encode_as_ids('Este es el librito de Job'))\n",
    "#print(sp_word.encode_as_pieces('Este es el librito de Job'))\n",
    "#print(sp_word.encode_as_ids('Este es el librito de Job'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SegmentaciÃ³n de palabras con SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–Este', 'â–es', 'â–el', 'â–librito', 'â–de', 'â–Job.']\n",
      "[298, 21, 6, 219, 3, 0]\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=labiblia.txt --model_prefix=m_word --model_type=word --vocab_size=2000')\n",
    "\n",
    "sp_word = spm.SentencePieceProcessor()\n",
    "sp_word.load('m_word.model')\n",
    "\n",
    "print(sp_word.encode_as_pieces('Este es el librito de  Job.'))  # '.' will not be one token.\n",
    "print(sp_word.encode_as_ids('Este es el libro de Job.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de prefijos  con SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–Este', 'â–es', 'â–el', 'â–li', 'b', 'ri', 'to', 'â–de', 'â–Job']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sp_word' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3353b8693aa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'm.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_as_pieces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Este es el librito de Job'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_as_pieces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Â¿Quieres saber cÃ³mo hacer una aplicaciÃ³n para Android Wear? Te lo explicamos &gt; http://t.co/IjgCIkigvm http://t.co/tUZhU3wrhx '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_as_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Este es el librito de Job'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sp_word' is not defined"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.train('--input=labiblia.txt --model_prefix=m --vocab_size=2000')\n",
    "\n",
    "# makes segmenter instance and loads the model file (m.model)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n",
    "print(sp.encode_as_pieces('Este es el librito de Job'))\n",
    "print(sp_word.encode_as_pieces('Â¿Quieres saber cÃ³mo hacer una aplicaciÃ³n para Android Wear? Te lo explicamos &gt; http://t.co/IjgCIkigvm http://t.co/tUZhU3wrhx '))\n",
    "print(sp.encode_as_ids('Este es el librito de Job'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo caracter con SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–', 'E', 's', 't', 'e', 'â–', 'e', 's', 'â–', 'e', 'l', 'â–', 'l', 'i', 'b', 'r', 'i', 't', 'o', 'â–', 'd', 'e', 'â–', 'J', 'o', 'b', '.']\n",
      "[3, 36, 7, 13, 4, 3, 4, 7, 3, 4, 11, 3, 11, 10, 19, 8, 10, 13, 6, 3, 12, 4, 3, 43, 6, 19, 20]\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=labiblia.txt --model_prefix=m_char --model_type=char --vocab_size=400')\n",
    "\n",
    "sp_char = spm.SentencePieceProcessor()\n",
    "sp_char.load('m_char.model')\n",
    "\n",
    "print(sp_char.encode_as_pieces('Este es el librito de Job.'))\n",
    "print(sp_char.encode_as_ids('Este es el librito de Job.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de usuario deifindo para usar con BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–', 't', 'hi', 's', '<sep>', 'â–', 'is', 'â–a', 'â–te', 's', 't', 'â–he', 'llo', 'â–', 'w', 'or', 'l', 'd', '<cls>']\n",
      "3\n",
      "4\n",
      "3= <sep>\n",
      "4= <cls>\n",
      "['â–', '<', 's', '>', 'T', 'w', '3', '3', 't', '<sep>', 'â–#', 'â–@', 'du', 'de', '_', 'rea', 'll', 'y', '<', 'un', 'k', '>', 'â–#', 'ha', 's', 'h', '_', 't', 'a', 'g', 'â–', '$', 'hi', 't', 'â–(', 'g', '@', 'y', ')', 'â–re', 't', 'ar', 'd', '#', 'd', 'â–@', 'du', 'de', '.', 'â–', 'ğŸ˜€ğŸ˜€', 'â–', '!', 'ğŸ˜€', 'a', 'b', 'c', 'â–', '%', 'ğŸ˜€', 'lo', 'l', 'â–#', 'ha', 'te', 'i', 't', 'â–#', 'ha', 'te', '.', 'i', 't', 'â–', '$', '%', '&', '/', 'â–f', '*', 'ck', '-', 'â–in', 'â–lo', 've', '_', 't', 'wi', 't', 't', 'er', '<cls>', '<', '/', 's', '>']\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=tweets_clean.txt --model_prefix=m_user --user_defined_symbols=<sep>,<cls> --vocab_size=2000')\n",
    "# ids are reserved in both mode.\n",
    "# <unk>=0, <s>=1, </s>=2, <sep>=3, <cls>=4\n",
    "# user defined symbols allow these symbol to apper in the text.\n",
    "sp_user = spm.SentencePieceProcessor()\n",
    "sp_user.load('m_user.model')\n",
    "print(sp_user.encode_as_pieces('this<sep> is a test hello world<cls>'))\n",
    "print(sp_user.piece_to_id('<sep>'))  # 3\n",
    "print(sp_user.piece_to_id('<cls>'))  # 4\n",
    "print('3=', sp_user.decode_ids([3]))  # decoded to <sep>\n",
    "print('4=', sp_user.decode_ids([4]))  # decoded to <cls>\n",
    "print(sp_user.encode_as_pieces('<s>Tw33t<sep> # @dude_really<unk> #hash_tag $hit (g@y) retard#d @dude. ğŸ˜€ğŸ˜€ !ğŸ˜€abc %ğŸ˜€lol #hateit #hate.it $%&/ f*ck- in love_twitter<cls></s>'))\n",
    "#print('0=', sp_user.decode_ids([0]))  # decoded to <cls>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–Â¿Quieres<sep>', 'â–saber', 'â–cÃ³mo', 'â–hacer', 'â–una', 'â–aplicaciÃ³n', 'â–para', 'â–Android', 'â–Wear?', 'â–Te', 'â–lo', 'â–explicamos', 'â–&gt;', 'â–http://t.co/IjgCIkigvmâ–http://t.co/tUZhU3wrhx']\n",
      "[102, 15, 7, 785, 3, 0]\n",
      "['â–@HyperKynetic', 'â–si', 'â–me', 'â–los', 'â–traes', 'â–a', 'â–la', 'â–te', 'â–querrÃ­a', 'â–para', 'â–siempre', 'â–ğŸ˜', 'â–que', 'â–en', 'â–#ayunas', 'â–por', 'â–sacarme', 'â–sangre', 'â–:(']\n",
      "['â–Tw33tâ–#â–@dude_reallyâ–#hash_tagâ–$hitâ–(g@y)â–retard#dâ–@dude.â–ğŸ˜€ğŸ˜€â–!ğŸ˜€abcâ–%ğŸ˜€lolâ–#hateitâ–#hate.itâ–$%&/â–f*ck-', 'â–in', 'â–love_twitter']\n",
      "['â–Cigaretteâ–smokeâ–containsâ–poisonousâ–elementsâ–thatâ–goâ–intoâ–ourâ–body', 'â–and', 'â–makeâ–usâ–sufferâ–fromâ–it,due', 'â–to', 'â–which', 'â–a', 'â–deadlyâ–diseaseâ–likeâ–cancer', 'â–is', 'â–born.â–Stopâ–smokingâ–tobacco!â–Pleaseâ–doâ–listen', 'â–to', 'â–satsangâ–dailyâ–onâ–#GodMorningFriday#,â–#Coronvirus']\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.train('--input=tweets_clean.txt --model_prefix=m_word --model_type=word --vocab_size=2000')\n",
    "\n",
    "sp_word = spm.SentencePieceProcessor()\n",
    "sp_word.load('m_word.model')\n",
    "\n",
    "print(sp_word.encode_as_pieces('Â¿Quieres<sep> saber cÃ³mo hacer una aplicaciÃ³n para Android Wear? Te lo explicamos &gt; http://t.co/IjgCIkigvm http://t.co/tUZhU3wrhx '))\n",
    "print(sp_word.encode_as_ids('Este es el libro de Job.'))\n",
    "print(sp_word.encode_as_pieces(\"@HyperKynetic si me los traes a la te querrÃ­a para siempre ğŸ˜ que  en #ayunas por sacarme sangre :( \"))\n",
    "print(sp_word.encode_as_pieces('Tw33t # @dude_really #hash_tag $hit (g@y) retard#d @dude. ğŸ˜€ğŸ˜€ !ğŸ˜€abc %ğŸ˜€lol #hateit #hate.it $%&/ f*ck- in love_twitter'))\n",
    "print(sp_word.encode_as_pieces('Cigarette smoke contains poisonous elements that go into our body and make us suffer from it,due to which a deadly disease like cancer is born. Stop smoking tobacco! Please do listen to satsang daily on #GodMorningFriday#, #Coronvirus'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
